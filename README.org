:PROPERTIES:
:header-args: :results verbatim :exports both
:END:
#+title: its-jointprobability
#+EXPORT_EXCLUDE_TAGS: noexport

A Bayesian approach to generating metadata for educational materials.

This project is primarily intended to be used as a microservice through the ~nix~ package. Additionally, it includes some CLI utilities in order to (re-) train the model for some data (data not included).

* Utils :noexport:
#+name: format-json
#+begin_src shell sh :var result="" :results verbatim
echo $result | json
#+end_src

#+name: format-prediction
#+begin_src python :var result="" :results verbatim output :session python-jointprobability-demo
import json
import pandas as pd
result_dict = json.loads(result)
df = pd.DataFrame.from_dict(result_dict["disciplines"]).set_index("name")
df = df.drop("id", axis=1)
df["prob_interval"] = df.apply(lambda x: [f"{y:g}" for y in x["prob_interval"]], axis=1)
print(df)
#+end_src

* Usage

** Service

With ~Nix~, no further installation is required to run the microservice. Simply run the following command:
#+begin_src shell
nix run github:openeduhub/its-jointprobability
#+end_src
or optionally, with CUDA support:
#+begin_src shell
nix run "github:openeduhub/its-jointprobability#with-cuda"
#+end_src

If the package has been installed locally, the service is also available as ~its-jointprobability~ from the command line.

For more information on configuration options, see
#+begin_src shell
nix run github:openeduhub/its-jointprobability -- --help
#+end_src

Once started, see the ~Swagger~ UI for documentation on the service.
It is located on =http://localhost:8080/docs= by default.

** Model Training

To retrain the model under some data, use the included ~retrain-model~ CLI tool, e.g. through
#+begin_src shell
nix shell github:openeduhub/its-jointprobability -c retrain-model <path/to/data-dir>
#+end_src
or, *highly recommended*, with CUDA:
#+begin_src shell
nix shell "github:openeduhub/its-jointprobability#with-cuda" -c retrain-model <path/to/data-dir>
#+end_src

The utility will look for =train_data= and =train_labels=, which are assumed to files that can be loaded through [[https://pytorch.org/docs/stable/generated/torch.load.html][torch.load]]. These should be (=float=-type) [[https://pytorch.org/docs/stable/tensors.html#torch.Tensor][torch.Tensor]] objects with the following content:
- ~train_data_labeled~ :: a two-dimensional =Tensor= where the first dimension corresponds to the individual documents to use for training and the second dimensions contains each document's content, encoded through their Bag-of-words representation.
- ~train_targets~ :: a two-dimensional =Tensor= where the first dimension corresponds to the individual documents to use for training and the second dimension encodes whether each document belongs to each discipline (=1.0= if it does, =0.0= otherwise).

Once the data has been loaded, the topic model will be trained (this will take a long time) and saved within the set directory under =prodslda=. If this file already exists, this step is skipped.

Finally, the Bayesian classification model is trained and saved under =classification=. At this point, some quality metrics will be computed for the model on the training data. If ~test_data_labeled~ and ~test_targets~ are present in the given directory (analogous to the training data), these quality metrics will also be computed for this testing data.

* Features & Demo of the Service
:PROPERTIES:
:header-args: :results verbatim :exports both :post format-json(result=*this*) :wrap src
:END:

** Ping

Once the service has started, we can ping it to check that it is responding to requests:
#+begin_src shell :post :exports both
curl -i -X GET http://localhost:8080/_ping
#+end_src

#+RESULTS:
#+begin_src
HTTP/1.1 200 OK
date: Mon, 15 Jan 2024 15:34:05 GMT
server: uvicorn
content-length: 4
content-type: application/json

null
#+end_src

** Discipline Prediction
:PROPERTIES:
:header-args: :results verbatim :exports both :post format-prediction(result=*this*) :wrap src
:END:

With the =/predict_disciplines= endpoint, we can send a text to be assigned school disciplines to the model. For readability, we only ask for the ten most relevant disciplines.

In addition to the identifiers of the predicted disciplines, we also get some diagnostics that help us understand whether this is a relevant match (in principle, all disciplines are always returned). Specifically, we gain two point-estimates (mean and median) for the probability of the discipline belonging to the given text, according to the model. We also get a credibility interval (by default 80%) on said probability.

In the example below, we get only one relevant school discipline, which is also the one we would be expecting for the text (Mathematics). Because the text is relatively short, the probability of this fit is still relatively low at around 50% to 60%.
#+begin_src shell :exports both
curl -X 'POST' \
  'http://localhost:8080/predict_disciplines' \
  -H 'Content-Type: application/json' \
  -d '{
  "text": "Der Satz des Pythagoras lautet: a^2 + b^2 = c^2. Er wird benutzt, um die Hypotenuse eines rechtwinkligen Dreiecks zu berechnen.",
  "num_predictions": "10"
}'
#+end_src

#+RESULTS:
#+begin_src
               mean_prob   median_prob               prob_interval
name                                                              
Mathematik      0.799448  9.997893e-01               [0.462171, 1]
Physik          0.106398  1.451062e-05    [1.9434e-15, 0.00891102]
Deutsch         0.010206  1.070765e-09  [1.71657e-23, 4.41269e-06]
Informatik      0.006888  1.940603e-10  [1.61938e-19, 9.59061e-06]
Englisch        0.007137  1.059791e-10  [1.77408e-19, 3.72526e-07]
Türkisch        0.009743  6.860193e-11   [3.9481e-22, 1.75739e-07]
Latein          0.009236  4.144079e-11  [8.26642e-24, 5.74176e-07]
Chemie          0.000418  1.035434e-11  [3.09142e-25, 3.67509e-08]
Geschichte      0.013904  6.290811e-12  [2.81917e-23, 6.01777e-07]
Medienbildung   0.017171  5.005991e-12  [6.40839e-24, 5.83756e-08]
#+end_src

Note that these predictions are stochastic, so another run on the same text may yield slightly different predictions:
#+begin_src shell :exports both
curl -X 'POST' \
  'http://localhost:8080/predict_disciplines' \
  -H 'Content-Type: application/json' \
  -d '{
  "text": "Der Satz des Pythagoras lautet: a^2 + b^2 = c^2. Er wird benutzt, um die Hypotenuse eines rechtwinkligen Dreiecks zu berechnen.",
  "num_predictions": "10"
}'
#+end_src

#+RESULTS:
#+begin_src
                          mean_prob   median_prob               prob_interval
name                                                                         
Mathematik                 0.686846  9.968573e-01              [0.0236473, 1]
Physik                     0.049169  1.260504e-06   [7.63135e-15, 0.00571426]
Deutsch                    0.011084  2.521037e-09  [2.03091e-20, 3.02907e-06]
Informatik                 0.038568  1.226665e-09  [4.33632e-21, 3.91388e-06]
Englisch                   0.012986  2.870477e-10  [2.69747e-24, 8.26626e-07]
Astronomie                 0.010568  3.391517e-11  [6.51483e-22, 6.93551e-08]
Türkisch                   0.000446  3.336027e-11  [6.02785e-22, 1.54782e-07]
Deutsch als Zweitsprache   0.002441  2.717240e-11  [2.27216e-27, 1.70444e-07]
Latein                     0.000040  2.278304e-11  [2.68186e-18, 5.94301e-08]
Spanisch                   0.012525  2.036649e-11  [1.78818e-21, 4.24471e-08]
#+end_src

To reduce this variance, we can increase the number of samples being drawn for the prediction. Note that the computation time is proportional to the number of such samples. By default, 100 samples are drawn.
#+begin_src shell :exports both
curl -X 'POST' \
  'http://localhost:8080/predict_disciplines' \
  -H 'Content-Type: application/json' \
  -d '{
  "text": "Der Satz des Pythagoras lautet: a^2 + b^2 = c^2. Er wird benutzt, um die Hypotenuse eines rechtwinkligen Dreiecks zu berechnen.",
  "num_predictions": "10",
  "num_samples": "2500"
}'
#+end_src

#+RESULTS:
#+begin_src
                          mean_prob   median_prob               prob_interval
name                                                                         
Mathematik                 0.754993  9.995947e-01               [0.149735, 1]
Physik                     0.091191  2.016381e-06   [5.05096e-24, 0.00776454]
Informatik                 0.012245  9.280623e-10  [2.13771e-23, 2.71682e-06]
Deutsch                    0.012949  6.357618e-10  [2.61476e-23, 2.58779e-06]
Englisch                   0.005998  1.282056e-10    [9.582e-25, 2.33286e-07]
Italienisch                0.005004  2.486902e-11  [1.09505e-25, 6.37075e-08]
Türkisch                   0.004970  2.454953e-11  [5.80388e-28, 7.53085e-08]
Latein                     0.004452  2.361180e-11  [1.70598e-28, 8.75516e-08]
Deutsch als Zweitsprache   0.005830  1.617866e-11   [3.4673e-32, 7.71424e-08]
Astronomie                 0.005397  6.791410e-12  [9.80316e-28, 2.64872e-08]
#+end_src

Second run, for comparison
#+begin_src shell :exports both
curl -X 'POST' \
  'http://localhost:8080/predict_disciplines' \
  -H 'Content-Type: application/json' \
  -d '{
  "text": "Der Satz des Pythagoras lautet: a^2 + b^2 = c^2. Er wird benutzt, um die Hypotenuse eines rechtwinkligen Dreiecks zu berechnen.",
  "num_predictions": "10",
  "num_samples": "2500"
}'
#+end_src

#+RESULTS:
#+begin_src
                          mean_prob   median_prob               prob_interval
name                                                                         
Mathematik                 0.734906  9.992125e-01              [0.0894214, 1]
Physik                     0.096943  1.915260e-06   [4.15132e-19, 0.00703591]
Informatik                 0.009247  1.309330e-09  [5.64667e-26, 1.98821e-06]
Deutsch                    0.014773  7.333741e-10  [1.10489e-24, 1.71488e-06]
Englisch                   0.008916  2.134399e-10  [2.09683e-24, 5.28894e-07]
Türkisch                   0.005436  2.777595e-11  [7.83329e-26, 1.35209e-07]
Italienisch                0.006591  2.531843e-11  [2.87492e-25, 9.23046e-08]
Deutsch als Zweitsprache   0.005947  1.152753e-11  [3.72586e-29, 6.60459e-08]
Geschichte                 0.014713  9.352101e-12  [7.45049e-27, 9.41332e-08]
Latein                     0.006336  8.414824e-12  [2.27865e-26, 3.50834e-08]
#+end_src

Note that the individual probabilities of all of the disciplines do not necessarily add up to 1. This is intended, as assigning a text multiple relevant disciplines is often desired. As an example, take the following paragraph taken from [[https://de.wikipedia.org/wiki/Deutschland][the German Wikipedia page on Germany]]. This is mostly about the history of Germany, but because it also covers relatively recent developments, it may also be relevant to politics. This is indeed what is predicted by the model.
#+begin_src shell :exports both
curl -X 'POST' \
  'http://localhost:8080/predict_disciplines' \
  -H 'Content-Type: application/json' \
  -d '{
  "text": "Die rasche Entwicklung vom Agrar- zum Industriestaat vollzog sich während der Gründerzeit in der zweiten Hälfte des 19. Jahrhunderts. Nach dem Ersten Weltkrieg wurde 1918 die Monarchie abgeschafft und die demokratische Weimarer Republik konstituiert. Ab 1933 führte die nationalsozialistische Diktatur zu politischer und rassistischer Verfolgung und gipfelte in der Ermordung von sechs Millionen Juden und Angehörigen anderer Minderheiten wie Sinti und Roma. Der vom NS-Staat 1939 begonnene Zweite Weltkrieg endete 1945 mit der Niederlage der Achsenmächte. Das von den Siegermächten besetzte Land wurde 1949 geteilt, nachdem bereits 1945 seine Ostgebiete teils unter polnische, teils sowjetische Verwaltungshoheit gestellt worden waren. Der Gründung der Bundesrepublik als demokratischer westdeutscher Teilstaat mit Westbindung am 23. Mai 1949 folgte die Gründung der sozialistischen DDR am 7. Oktober 1949 als ostdeutscher Teilstaat unter sowjetischer Hegemonie. Die innerdeutsche Grenze war nach dem Berliner Mauerbau (ab 13. August 1961) abgeriegelt. Nach der friedlichen Revolution in der DDR 1989 erfolgte die Lösung der deutschen Frage durch die Wiedervereinigung beider Landesteile am 3. Oktober 1990, womit auch die Außengrenzen Deutschlands als endgültig anerkannt wurden. Durch den Beitritt der fünf ostdeutschen Länder sowie die Wiedervereinigung von Ost- und West-Berlin zur heutigen Bundeshauptstadt zählt die Bundesrepublik Deutschland seit 1990 sechzehn Bundesländer.",
  "num_predictions": "10",
  "num_samples": "2500"
}'
#+end_src

#+RESULTS:
#+begin_src
                          mean_prob   median_prob               prob_interval
name                                                                         
Geschichte                 1.000000  1.000000e+00                      [1, 1]
Politik                    0.278964  1.378762e-02      [5.26298e-15, 0.84083]
Deutsch                    0.209880  1.539857e-03     [6.89587e-17, 0.517412]
Medienbildung              0.078251  4.870991e-06   [8.24893e-18, 0.00630399]
Englisch                   0.030032  8.268547e-08  [4.32991e-21, 0.000107654]
Latein                     0.025899  3.461025e-09  [3.94445e-26, 2.35824e-05]
Pädagogik                  0.030022  1.091193e-09  [1.64721e-28, 8.17412e-06]
Physik                     0.016192  2.443612e-10  [8.27201e-28, 1.11231e-06]
Französisch                0.013344  1.442791e-10  [1.86042e-30, 5.94394e-07]
Deutsch als Zweitsprache   0.005190  1.181849e-10   [3.4114e-25, 2.85259e-07]
#+end_src

* Notes / Limitations

** RAM Usage

The service requires roughly 2GB of RAM to operate. This usage should be static with time.

* Installation (through ~Nix Flakes~)

Add this repository to your Flake inputs. This may look like this:
#+begin_src nix
{
  inputs = {
    its-jointprobability = {
      url = "github:openeduhub/its-jointprobability";
      # optional if using as application, required if using as library
      nixpkgs.follows = "nixpkgs"; 
    };
  };
}
#+end_src

The micro-service is provided both as a ~nixpkgs~ overlay and as an output (~packages.${system}.its-jointprobability~). Thus, it may be included through
#+begin_src nix
{
  outputs = { self, nixpkgs, its-jointprobability, ... }:
    let
      system = "x86_64-linux";
      pkgs =
        (nixpkgs.legacyPackages.${system}.extend
          its-jointprobability.overlays.default);
    in
    { ... };
}
  
#+end_src

The Python library is provided as an output (~lib.${system}.its-jointprobability~). Note that this is a function mapping a Python package (e.g. ~pkgs.python310~) to the library. Its inclusion may look like this:
#+begin_src nix
{
  outputs = { self, nixpkgs, its-jointprobability, ... }:
    let
      system = "x86_64-linux";
      pkgs = nixpkgs.legacyPackages.${system};
      
      python-with-packages =
        pkgs.python310.withPackages (py-pkgs: [
          # some example packages
          py-pkgs.numpy
          py-pkgs.pandas
          # the its-jointprobability library
          (its-jointprobability.lib.${system}.its-jointprobability py-pkgs)
        ]);
    in
    { ... };
}
#+end_src
